# ===============================================================
# TRAINING CONTROL PANEL
# This file defines how the next training session will behave.
# EDIT
# ===============================================================

# --- Task Identification ---
experiment_name: "gold_price_prediction_v1"
ticker: "GOLD"          # The ticker symbol to train on
timeframe: "1d"         # Options: 1m, 5m, 1h, 1d

# --- Data Handling ---
dataset:
  # The path is relative to the PROJECT ROOT (handled by our Path Manager)
  raw_data_file: "data/raw/market_data/GOLD_1d.csv"
  include_news_sentiment: true
  sequence_length: 60    # Look-back window (e.g., use 60 days to predict day 61)
  train_test_split: 0.8  # 80% for training, 20% for testing
  shuffle: false         # Always false for Time-Series data to avoid look-ahead bias

# --- Model Architecture ---
# You can change these to try different "shapes" of the brain
model:
  type: "BiLSTM"         # Options: LSTM, BiLSTM, GRU
  hidden_layers: [128, 64] # Neurons per layer
  dropout_rate: 0.2      # Helps prevent "overfitting" (memorizing data)
  activation: "relu"

# --- Training Hyperparameters ---
# These are the "knobs" the user turns to optimize performance
hyperparameters:
  epochs: 100            # How many times to see the full dataset
  batch_size: 32         # How many samples to see before updating weights
  learning_rate: 0.001   # The "speed" of learning
  optimizer: "adam"      # Standard for deep learning
  loss_function: "mse"   # Mean Squared Error for price regression

# --- Environment & Safety ---
environment:
  device: "auto"         # 'cuda' for GPU, 'cpu' for processor, 'auto' for best available
  early_stopping: true   # Stop training early if accuracy stops improving
  patience: 10           # Wait 10 epochs before stopping early

# --- Persistence ---
# Where the model "lives" after training is done
output:
  export_folder: "models/GOLD_BiLSTM_latest"
  overwrite: true   # Only save the version with the lowest error